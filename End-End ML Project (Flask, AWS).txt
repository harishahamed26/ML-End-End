
Refrence Playlist (https://www.youtube.com/playlist?list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)

Tutorial 1: 

1 - Set up github
	a) new environment
		1. conda create -p venv python=3.8 -y
		2. conda activate venv/
	
	Making the changes sync to github
	echo "# ML-End-End" >> README.md
	git init
	git add README.md
	git commit -m "first commit"
	git branch -M main
	git remote add origin https://github.com/harishahamed26/ML-End-End.git
	git push -u origin main

	add git ignore in the repositiory. - > add file -> filename .gitignore and python as template
	To update in vs code:	git pull  

	b) setup.py
		Used to create a package and even deploy in pypi (just like pip install seaborn)
		create a folder src and create __init__.py (to find the src as package)
	c) requiremenet.txt
		write all the packages
		-e . ( [-e . ]to automate to run the setup.py whenever the new requirements are added

Once everything done:

	- pip install -r requirements.txt
and so  ->  ml_project.egg-info or mlproject.egg-info will be created included with dependency_links.txt, PKG-INFO, requires.txt, SOURCES.txt and top_level.txt

final:

git add . # to add all the files in git
git status # to check the status 
git commit -m "setup and requirements commit" # to commit the changes
git push -u origin main # to push into github


------------------------------------------------------------------------------------------------

Tutorial 2:

Project Structure
Logging
Exceptional Handling

1. create folder name in src : components and create __init__.py file
	Components: [components are noting but models]
		- __init__.py
		- data_ingestion.py [ ingesting the data ]
		- data_transformation.py [ transforming the data ]
		- model_trainer.py [ training the models ]
2. create folder name in src : pipeline and create __init__.py file
	pipeline
		- __init__.py
		- train_pipeline.py [ pipeline to train the data ]
		- predict_pipeline.py [ pipeline to predict the data ]
	
3. create file name in src : logger.py [ for record logging ]
4. create file name in src : exception.py [ for exception handling ]
5. create file name in src : util.py [ common functions ]


exception.py:
	We can actually use the exceptional documentations

logger.py
	We can actually use the logger documentations


final:

git add . # to add all the files in git
git status # to check the status 
git commit -m "setup and requirements commit" # to commit the changes
git push -u origin main # to push into github


------------------------------------------------------------------------------------------------

Tutorial 3:

Problem Statement: 

This project understands how the student's performance (test scores) is affected by other variables such as Gender, Ethnicity, Parental level of education, Lunch and Test preparation course.

Data Source: https://www.kaggle.com/datasets/spscientist/students-performance-in-exams?resource=download


1. create folder name in main project : notebook [ To perform the EDA and Evaluation]
	- EDA.ipynb [ To perfom the Exploratory Data Analysis ]
	- model_training.ipynb [ To train and assess the model ]
2. create folder name in main project : data [ To store the data.csv/ data]

final:

git add . # to add all the files in git
git status # to check the status 
git commit -m "setup and requirements commit" # to commit the changes
git push -u origin main # to push into github

Notes:

xgboost will not work for m1 mac so we have to install -> brew install libomp
else we will get error message as:

raise XGBoostError(
xgboost.core.XGBoostError: 
XGBoost Library (libxgboost.dylib) could not be loaded.
Likely causes:
  * OpenMP runtime is not installed
    - vcomp140.dll or libgomp-1.dll for Windows
    - libomp.dylib for Mac OSX
    - libgomp.so for Linux and other UNIX-like OSes
    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.

  * You are running 32-bit Python on a 64-bit OS

------------------------------------------------------------------------------------------------

Tutorial 4:

Data Ingestion Implementation

Updated the data ingestion python file 


final:

git add . # to add all the files in git
git status # to check the status 
git commit -m "setup and requirements commit" # to commit the changes
git push -u origin main # to push into github



------------------------------------------------------------------------------------------------

Tutorial 5:


Data Transformation


Updated the data ingestion python file once data transformation file completed and create a preprocessor.pkl (pickle file)


final:

git add . # to add all the files in git
git status # to check the status 
git commit -m "setup and requirements commit" # to commit the changes
git push -u origin main # to push into github



------------------------------------------------------------------------------------------------

Tutorial 6:


Model Training and Evaluating

Updated the data ingestion python file once model training and evaluating model_trainer file completed and create a model.pkl (pickle file) for best model

final:

git add . # to add all the files in git
git status # to check the status 
git commit -m "setup and requirements commit" # to commit the changes
git push -u origin main # to push into github

------------------------------------------------------------------------------------------------

Tutorial 7:


Model Hyperparameter tuning


Updated the data model_trainer python file 

final:

git add . # to add all the files in git
git status # to check the status 
git commit -m "setup and requirements commit" # to commit the changes
git push -u origin main # to push into github


------------------------------------------------------------------------------------------------

Tutorial 8:

Create Prediction Pipeline Using Flask Web


1. Create application.py in the project main folder

final:

git add . # to add all the files in git
git status # to check the status 
git commit -m "setup and requirements commit" # to commit the changes
git push -u origin main # to push into github


------------------------------------------------------------------------------------------------

Tutorial 9 :

AWS deploy (Code pipeline)


1. create a eb.extension folder and inside create python.config file to config the AWS elastic beanstalk

2. push the updated code to https://github.com/harishahamed26

3. create an environment in AWS EB 

4. Connect with the github repo  ( which act as a code pipeline to AWS EB )

5. click deploy 

Note:
as the changes made in the code. we need to manually deploy the changes again in AWS EB 


------------------------------------------------------------------------------------------------

Tutorial 10 : Reference (https://www.youtube.com/watch?v=8vmKtS8W7IQ)

Docker for deployment : which is more essential to deploy application for CI/CD 


Docker file :

FROM <baseImage> eg: python 3.8 -alpine
COPY <. /<folder-name> > eg: . /app
WORKDIR < /<folder-name> > eg: /app
RUN <run-prerequisite> eg: pip install -r requirements.txt
CMD < cmds> eg: python app.py

save and run the below cmd to create the image

cmd -> docker build -t <image-name> . eg: docker build -t welcome-app .

to check the images: cmd -> docker images

run docker image to container: (host port and container port )

cmd -> docker run -p <port>:<container-port> <image-name> eg: docker run -p 5000:5000 welcome-app

once done. switch to new cmd

to see the no of containers running: cmd -> docker ps 

to stop container: cmd -> docker stop <container-id>


To push into the docker hug

cmd -> docker login

usrname and password



to remove the docker image

cmd -> docker image rm -f <image-name>
 or docker tag <app-name> <new-app-name> eg: docker tage welcome-app welcome-app1

to build

cmd -> docker build -t <username/appname> eg: docker build -t harish/welcome-app


to push

cmd-> docker push <name>:latest eg: docker push welcome-app:latest

to run

cmd-> docker run -d -p 5000:5000 <name>:latest


for docker compose (docker-compose.yml)

cmd -> docker compose up


to stop compose stop:

cmd -> docker compose stop




------------------------------------------------------------------------------------------------










